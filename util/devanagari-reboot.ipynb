{
 "metadata": {
  "name": "",
  "signature": "sha256:6dc0bbc8bc0fc7bc3bd81b1359181f3e1de3d9c4f323d359c21464747a5ced1a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# -*- coding: utf-8 -*-\n",
      "# <nbformat>3.0</nbformat>\n",
      "\n",
      "# <codecell>\n",
      "\n",
      "# This fetches the current poems from a private Drupal site accessed via xmlrpc\n",
      "\n",
      "import xmlrpcsettings # this holds a few private settings\n",
      "import xmlrpclib\n",
      "import re\n",
      "import json\n",
      "import urllib\n",
      "import codecs\n",
      "\n",
      "assert xmlrpcsettings.ursite\n",
      "assert xmlrpcsettings.inbookurl\n",
      "\n",
      "proxy = xmlrpclib.Server(xmlrpcsettings.ursite)\n",
      "\n",
      "open_url = urllib.urlopen(xmlrpcsettings.inbookurl)\n",
      "nds=json.load(open_url)\n",
      "items = nds['Items']\n",
      "inbook = []\n",
      "for i in items:\n",
      "    inbook.append( proxy.node.retrieve(i['node']['nid']) )\n",
      "assert len(inbook)==30\n",
      "\n",
      "# <codecell>\n",
      "\n",
      "import myparser\n",
      "\n",
      "# <codecell>\n",
      "\n",
      "def get_trans(n):\n",
      "    return n['body']['und'][0]['value']\n",
      "\n",
      "def clean_trans(x):\n",
      "\n",
      "    x=x.replace(' -o- ','-o-') # join these for now to better extract izafats\n",
      "    x=x.replace('!','')\n",
      "    x=x.replace('^','')\n",
      "    x=x.replace(',',' ')\n",
      "    x=x.replace('?',' ')\n",
      "    x=x.replace('--',' ')\n",
      "    x=x.replace('\\t',' ')\n",
      "    x=x.replace('\"',' ')\n",
      "    x=re.sub('\\s+',' ',x)\n",
      "\n",
      "    x=re.sub(\"^\\s+\",'',x)\n",
      "    x=re.sub(\"\\s+$\",'',x)\n",
      "#    x=x.replace('_','-')\n",
      "    return x\n",
      "def get_lines(x):\n",
      "    return x.split('\\n') # may need to account for extra empty lines at end\n",
      "\n",
      "# <codecell>\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "chars = \"[a-z\\(\\):\\-;\\._']+?\"\n",
      "z= '('+chars+'(?:-e '+chars+')*'+')'+'(?: |$)'                                              \n",
      "pattern = re.compile(z,re.UNICODE)\n",
      "\n",
      "\n",
      "# <codecell>\n",
      "\n",
      "words = {}\n",
      "def clean_word(x): # remove brackets\n",
      "    if re.match('\\([^(]',x):\n",
      "         x = x[1:]\n",
      "#         print x\n",
      "    if re.search('[^)]\\)$',x):\n",
      "        x = x[0:-1]\n",
      "    if re.search(':$',x):\n",
      "         x = x[0:-1]\n",
      "    x=re.sub('\\.\\.\\.$','',x)\n",
      "\n",
      "#        print x\n",
      "    return x\n",
      "for i,n in enumerate(inbook):\n",
      "    x = clean_trans(get_trans(n))\n",
      "    lines=get_lines(x)\n",
      "#    print lines\n",
      "    for line_id,l in enumerate(lines):\n",
      "        matches = []\n",
      "        for match in pattern.finditer(l):\n",
      "            matches.append(match.group(0))\n",
      "            w = match.group(1)\n",
      "            w = clean_word(w)\n",
      "            if w in words:\n",
      "                words[w].append((i,line_id))\n",
      "            else:\n",
      "                words[w] = [(i,l)]\n",
      "        assert l==''.join(matches)\n",
      "\n",
      "        \n",
      "\n",
      "# <codecell>\n",
      "\n",
      "re.match('\\([^(]','((')\n",
      "\n",
      "# <codecell>\n",
      "import csv\n",
      "def unicode_csv_reader(utf8_data, dialect=csv.excel, **kwargs):\n",
      "      csv_reader = csv.reader(utf8_data, dialect=dialect, **kwargs)\n",
      "      for row in csv_reader:\n",
      "          yield [unicode(cell, 'utf-8') for cell in row]\n",
      "old = unicode_csv_reader('rashid-dev.csv')\n",
      "\n",
      "def update_okays():\n",
      "    okay_dev={}\n",
      "    reader = unicode_csv_reader(open('okay-dev.csv'))\n",
      "    for transliteration,devanagari in reader:\n",
      "        okay_dev[transliteration]=devanagari\n",
      "\n",
      "    reader = unicode_csv_reader(open('rashid-dev.csv'))\n",
      "    for ok,devanagari,transliteration,count,meter,replacement in reader:\n",
      "        if ok=='x':\n",
      "            okay_dev[transliteration] = devanagari\n",
      "\n",
      "    with codecs.open('okay-dev.csv','w','utf8') as f:        \n",
      "        for k,v in okay_dev.iteritems():\n",
      "            f.write(k+u','+v+'\\n') #generate new csv\n",
      "#            print k,v\n",
      "    return okay_dev\n",
      "\n",
      "def regenerate():\n",
      "    \n",
      "    diacritics = myparser.Parser('diacritics.yaml')\n",
      "    devanagari = myparser.Parser('devanagari.yaml')\n",
      "    urdu = myparser.Parser('urdu.yaml')\n",
      "    um=myparser.Parser('urdu-meter.yaml')\n",
      "    okays = update_okays()\n",
      "    print 'okays is ',len(okays)\n",
      "\n",
      "    output=u\"ok,devanagari,transliteration,count,meter,replacement\\n\"\n",
      "#    for w in sorted(words, key=lambda k: len(words[k]), reverse=True):\n",
      "    for w in sorted(words, key=lambda k: len(words[k]), reverse=True):\n",
      "\n",
      "   #     print w\n",
      "        dev = devanagari.parse(w)\n",
      "        okay=''\n",
      "        if w in okays:\n",
      " #          print 'yes'\n",
      "            if okays[w] ==dev:\n",
      "                okay='x'\n",
      "            else:\n",
      "                print 'warning ',w,' is no longer', okays[w]\n",
      "                \n",
      "        output+=okay+','+dev+','+w+','+str(len(words[w]))+','+um.parse(w)+','+'\\n'\n",
      "\n",
      "        #print devanagari.parse(w),w,len(words[w])#print '\\n'.join(words.keys())\n",
      "    import codecs\n",
      "    with codecs.open('rashid-dev.csv','w','utf-8') as f:\n",
      "        f.write(output)\n",
      "regenerate()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<consonant> <long_vowel> <s> + <t> <long_vowel>\n",
        "<consonant> <long_vowel> <consonant> + <consonant> <long_vowel>\n",
        "<consonant> <vowel> <vowel_nasal> <consonant> + <consonant> <long_vowel>\n",
        "<consonant> <short_vowel> <h_char> + <consonant>\n",
        "<consonant> <short_vowel> <consonant> + <consonant> <long_vowel>\n",
        "<wb> <vowel> <consonant> + <consonant> <vowel>\n",
        "<wb> <consonant> <short_vowel> <z> + <z> <short_vowel> <consonant>\n",
        "<wb> <consonant> <short_vowel> <consonant> + <consonant> <short_vowel> <consonant>\n",
        "<n> + <consonant>\n",
        "<wb> <long_vowel> <consonant> + <consonant> <long_vowel>\n",
        "<consonant> + <consonant>\n",
        "<k> + <k_group>\n",
        "<ch> + <ch_group>\n",
        "<wb> <consonant> <long_vowel> <consonant> + <consonant> <long_vowel>\n",
        "<wb> <consonant> <long_vowel> <consonant> + <consonant> <short_vowel> <consonant>\n",
        "<wb> <consonant> <short_vowel> <sibilant> + <consonant> <wb>\n",
        "<wb> <consonant> <short_vowel> <consonant> + <consonant> <wb>\n",
        "<wb> <short_vowel> <consonant> + <consonant> <long_vowel> <wb>\n",
        "<wb> <short_vowel> <consonant> + <consonant> <long_vowel> <consonant> <wb>\n",
        "<wb> <short_vowel> <consonant> <short_vowel> <consonant> + <consonant> <short_vowel> <consonant>\n",
        "<vowel> <n> + <consonant>\n",
        "<consonant> + <n> <aa> <wb>\n",
        "<consonant> + <n> <e> <wb>\n",
        "<consonant> + <t> <aa> <wb>\n",
        "<consonant> + <t> <e> <wb>\n",
        "<consonant> + <t> <ii> <wb>\n",
        "<consonant> + <ain> <short_vowel> <wb>\n",
        "<short_vowel> <ain> + <consonant>\n",
        "<wb> <consonant> <short_vowel> + <consonant> <long_vowel>\n",
        "<wb> <consonant> <short_vowel> <consonant> + <consonant> <long_vowel>\n",
        "<consonant> + <consonant>\n",
        "okays is "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2326\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " tamaashaa-gah-e laalah-zaar  is no longer \u0924\u092e\u093e\u0936\u093e\u0917\u0939\u2010\u090f\u2010\u0932\u093e\u0932\u093e\u091c\u093c\u093e\u0930\n",
        "warning  aarzuu  is no longer \u0906\u0930\u094d\u091c\u093c\u0942\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " aasmaa;n  is no longer \u0906\u0938\u094d\u092e\u093e\u0901\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " daastaa;n  is no longer \u0926\u093e\u0938\u094d\u0924\u093e\u0901\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rassii  is no longer \u0930\u0938\u094d\u0938\u0940\n",
        "warning  u;t;thii  is no longer \u0909\u091f\u094d\u0920\u0940\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " aarzuu))e;n  is no longer \u0906\u0930\u094d\u091c\u093c\u0941\u090f\u0901\n",
        "warning  aarzuu))o;n  is no longer \u0906\u0930\u094d\u091c\u093c\u0941\u0913\u0902\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " tumhe;n  is no longer \u0924\u0941\u092e\u094d\u0939\u0947\u0902\n",
        "warning  patthar  is no longer \u092a\u0924\u094d\u0925\u0930\n",
        "warning  justajuu  is no longer \u091c\u0941\u0938\u094d\u0924\u091c\u0942\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " mallaa;h  is no longer \u092e\u0932\u094d\u0932\u093e\u0939\n",
        "warning  takraar  is no longer \u0924\u0915\u094d\u0930\u093e\u0930\n",
        "warning  raste  is no longer \u0930\u0938\u094d\u0924\u0947\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dukkaan  is no longer \u0926\u0941\u0915\u094d\u0915\u093e\u0928\n",
        "warning  ishtiraakii  is no longer \u0907\u0936\u094d\u0924\u093f\u0930\u093e\u0915\u0940\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " afrang  is no longer \u0905\u092b\u093c\u094d\u0930\u0902\u0917\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " mastuur  is no longer \u092e\u0938\u094d\u0924\u0942\u0930\n",
        "warning  la;z;zaat  is no longer \u0932\u091c\u093c\u094d\u091c\u093c\u093e\u0924\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " i;xtiyaar  is no longer \u0907\u0916\u093c\u094d\u0924\u093f\u092f\u093e\u0930\n",
        "warning  vaa-raftah  is no longer \u0935\u093e\u0930\u092b\u093c\u094d\u0924\u093e\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " tajallii  is no longer \u0924\u091c\u0932\u094d\u0932\u0940\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " ;hurriyat  is no longer \u0939\u0941\u0930\u094d\u0930\u093f\u092f\u0924\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " vaa-bastah  is no longer \u0935\u093e\u092c\u0938\u094d\u0924\u093e\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " ta.sviir  is no longer \u0924\u0938\u094d\u0935\u0940\u0930\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rakkhaa  is no longer \u0930\u0915\u094d\u0916\u093e\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " :tayyaarah  is no longer \u0924\u092f\u094d\u092f\u093e\u0930\u093e\n",
        "warning  tumhaare  is no longer \u0924\u0941\u092e\u094d\u0939\u093e\u0930\u0947\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " ((ushshaaq  is no longer \u0909\u0936\u094d\u0936\u093e\u0915\u093c\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " parastaar  is no longer \u092a\u0930\u0938\u094d\u0924\u093e\u0930\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " tumhaaraa  is no longer \u0924\u0941\u092e\u094d\u0939\u093e\u0930\u093e\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dostii  is no longer \u0926\u094b\u0938\u094d\u0924\u0940\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " avvalii;n  is no longer \u0905\u0935\u094d\u0935\u0932\u0940\u0902\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " ((a:t:taar  is no longer \u0905\u0924\u094d\u0924\u093e\u0930\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " ashko;n  is no longer \u0905\u0936\u094d\u0915\u094b\u0902\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dara;xto;n  is no longer \u0926\u0930\u0916\u093c\u094d\u0924\u094b\u0902\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " paristaar  is no longer \u092a\u0930\u093f\u0938\u094d\u0924\u093e\u0930\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " royaa-e aasmaanii  is no longer \u0930\u094b\u092f\u093e\u2010\u090f\u2010\u0906\u0938\u094d\u092e\u093e\u0928\u0940\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " shabistaano;n  is no longer \u0936\u092c\u093f\u0938\u094d\u0924\u093e\u0928\u094b\u0902\n",
        "warning  himmat-e naa-;xvub  is no longer \u0939\u093f\u092e\u094d\u092e\u0924\u2010\u090f\u2010\u0928\u093e\u0916\u093c\u0941\u092c\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " tumhaarii  is no longer \u0924\u0941\u092e\u094d\u0939\u093e\u0930\u0940\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " ittifaaqaat  is no longer \u0907\u0924\u094d\u0924\u093f\u092b\u093c\u093e\u0915\u093c\u093e\u0924\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " gulistaa;n  is no longer \u0917\u0941\u0932\u093f\u0938\u094d\u0924\u093e\u0901\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " .sub;he;n  is no longer \u0938\u0941\u092c\u094d\u0939\u0947\u0902\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " roz-e avvalii;n  is no longer \u0930\u094b\u091c\u093c\u2010\u090f\u2010\u0905\u0935\u094d\u0935\u0932\u0940\u0902\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " joyandah-e tamkiin  is no longer \u091c\u094b\u092f\u0902\u0926\u0939\u2010\u090f\u2010\u0924\u092e\u0915\u0940\u0928\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " shab-e raftah  is no longer \u0936\u092c\u2010\u090f\u2010\u0930\u092b\u093c\u094d\u0924\u093e\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " u;t;the;n  is no longer \u0909\u091f\u094d\u0920\u0947\u0902\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " aarzuu-e vi.saal-e ma((nii  is no longer \u0906\u0930\u094d\u091c\u093c\u0942\u2010\u090f\u2010\u0935\u093f\u0938\u093e\u0932\u2010\u090f\u2010\u092e\u093e\u0928\u0940\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " sar-e bistar-e sanjaab  is no longer \u0938\u0930\u2010\u090f\u2010\u092c\u093f\u0938\u094d\u0924\u0930\u2010\u090f\u2010\u0938\u0902\u091c\u093e\u092c\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " ummiid  is no longer \u0909\u092e\u094d\u092e\u0940\u0926\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " ;xvaab-e la;z;zat-aagii;n  is no longer \u0916\u093c\u094d\u0935\u093e\u092c\u2010\u090f\u2010\u0932\u091c\u093c\u094d\u091c\u093c\u0924\u0906\u0917\u0940\u0902\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " afrangii  is no longer \u0905\u092b\u093c\u094d\u0930\u0902\u0917\u0940\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " ishtiraak-e giraa;n-bahaa  is no longer \u0907\u0936\u094d\u0924\u093f\u0930\u093e\u0915\u2010\u090f\u2010\u0917\u093f\u0930\u093e\u0901\u092c\u0939\u093e\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " la;z;zat-e tasliim  is no longer \u0932\u091c\u093c\u094d\u091c\u093c\u0924\u2010\u090f\u2010\u0924\u0938\u094d\u0932\u0940\u092e\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " sabzo;n  is no longer \u0938\u092c\u094d\u091c\u093c\u094b\u0902\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dosto;n  is no longer \u0926\u094b\u0938\u094d\u0924\u094b\u0902\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " lab-haa-e azal-tishnah  is no longer \u0932\u092c\u0939\u093e\u2010\u090f\u2010\u0905\u091c\u093c\u0932\u0924\u093f\u0936\u094d\u0928\u093e\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " aarzuu-mandii  is no longer \u0906\u0930\u094d\u091c\u093c\u0942\u092e\u0902\u0926\u0940\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " farishto;n  is no longer \u092b\u093c\u0930\u093f\u0936\u094d\u0924\u094b\u0902\n",
        "warning  kanakhyo;n  is no longer \u0915\u0928\u0916\u094d\u092f\u094b\u0902\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " ((aruus-e javaa;n-saal-e fardaa  is no longer \u0905\u0930\u0942\u0938\u2010\u090f\u2010\u091c\u0935\u093e\u0901\u0938\u093e\u0932\u2010\u090f\u2010\u092b\u093c\u0930\u094d\u0926\u093e\n",
        "warning  shaqiiq-o-nastaran  is no longer \u0936\u0915\u093c\u0940\u0915\u093c\u2010\u0913\u2010\u0928\u0938\u094d\u0924\u0930\u0928\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " gumbado;n  is no longer \u0917\u0941\u092e\u094d\u092c\u0926\u094b\u0902\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " aastaa;n  is no longer \u0906\u0938\u094d\u0924\u093e\u0901\n",
        "warning  ba;xshe;n  is no longer \u092c\u0916\u093c\u094d\u0936\u0947\u0902\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " himmat-e yazdaa;n  is no longer \u0939\u093f\u092e\u094d\u092e\u0924\u2010\u090f\u2010\u092f\u091c\u093c\u094d\u0926\u093e\u0901\n",
        "warning  raaste  is no longer \u0930\u093e\u0938\u094d\u0924\u0947\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dara;xshaa;n  is no longer \u0926\u0930\u0916\u093c\u094d\u0936\u093e\u0901\n",
        "warning  la;z;zato;n  is no longer \u0932\u091c\u093c\u094d\u091c\u093c\u0924\u094b\u0902\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " ak;sar  is no longer \u0905\u0915\u094d\u0938\u0930\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " shab-haa-e zimistaa;n  is no longer \u0936\u092c\u0939\u093e\u2010\u090f\u2010\u091c\u093c\u093f\u092e\u093f\u0938\u094d\u0924\u093e\u0901\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " garmaa))e;nge  is no longer \u0917\u0930\u094d\u092e\u093e\u090f\u0902\u0917\u0947\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " patto;n  is no longer \u092a\u0924\u094d\u0924\u094b\u0902\n",
        "warning  shabistaa;n  is no longer \u0936\u092c\u093f\u0938\u094d\u0924\u093e\u0901\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " daste  is no longer \u0926\u0938\u094d\u0924\u0947\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " ;xvurshiid  is no longer \u0916\u093c\u0941\u0930\u094d\u0936\u0940\u0926\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " :tilismii  is no longer \u0924\u093f\u0932\u093f\u0938\u094d\u092e\u0940\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " parasto;n  is no longer \u092a\u0930\u0938\u094d\u0924\u094b\u0902\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " istibdaad  is no longer \u0907\u0938\u094d\u0924\u093f\u092c\u0926\u093e\u0926\n",
        "warning  alvand  is no longer \u0905\u0932\u094d\u0935\u0902\u0926\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " be-i;xtiyaarii  is no longer \u092c\u0947\u0907\u0916\u093c\u094d\u0924\u093f\u092f\u093e\u0930\u0940\n",
        "warning  aa;gosh-e aarzuu-mand  is no longer \u0906\u0917\u093c\u094b\u0936\u2010\u090f\u2010\u0906\u0930\u094d\u091c\u093c\u0942\u092e\u0902\u0926\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " uftaadah  is no longer \u0909\u092b\u093c\u094d\u0924\u093e\u0926\u093e\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dushmano;n  is no longer \u0926\u0941\u0936\u094d\u092e\u0928\u094b\u0902\n",
        "warning  rakkhii  is no longer \u0930\u0915\u094d\u0916\u0940\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " havas-parastii  is no longer \u0939\u0935\u0938\u092a\u0930\u0938\u094d\u0924\u0940\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " ((ishqii  is no longer \u0907\u0936\u094d\u0915\u093c\u0940\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " ummiid-e zifaaf  is no longer \u0909\u092e\u094d\u092e\u0940\u0926\u2010\u090f\u2010\u091c\u093c\u093f\u092b\u093c\u093e\u092b\u093c\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " pastiyo;n  is no longer \u092a\u0938\u094d\u0924\u093f\u092f\u094b\u0902\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " gum-gashtah  is no longer \u0917\u0941\u092e\u0917\u0936\u094d\u0924\u093e\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " aarzuu-e saliim  is no longer \u0906\u0930\u094d\u091c\u093c\u0942\u2010\u090f\u2010\u0938\u0932\u0940\u092e\n",
        "warning "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " jalvah-gah-e raaz  is no longer \u091c\u0932\u0935\u093e\u0917\u0939\u2010\u090f\u2010\u0930\u093e\u091c\u093c\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "devanagari = myparser.Parser('devanagari.yaml')\n",
      "print devanagari.parse(' tah ')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<consonant> <long_vowel> <s> + <t> <long_vowel>\n",
        "<consonant> <long_vowel> <consonant> + <consonant> <long_vowel>\n",
        "<consonant> <vowel> <vowel_nasal> <consonant> + <consonant> <long_vowel>\n",
        "<consonant> <short_vowel> <h_char> + <consonant>\n",
        "<consonant> <short_vowel> <consonant> + <consonant> <long_vowel>\n",
        "<wb> <vowel> <consonant> + <consonant> <vowel>\n",
        "<wb> <consonant> <short_vowel> <z> + <z> <short_vowel> <consonant>\n",
        "<wb> <consonant> <short_vowel> <consonant> + <consonant> <short_vowel> <consonant>\n",
        "<n> + <consonant>\n",
        "<wb> <long_vowel> <consonant> + <consonant> <long_vowel>\n",
        "<consonant> + <consonant>\n",
        "<k> + <k_group>\n",
        "<ch> + <ch_group>\n",
        "<wb> <consonant> <long_vowel> <consonant> + <consonant> <long_vowel>\n",
        "<wb> <consonant> <long_vowel> <consonant> + <consonant> <short_vowel> <consonant>\n",
        "<wb> <consonant> <short_vowel> <sibilant> + <consonant> <wb>\n",
        "<wb> <consonant> <short_vowel> <consonant> + <consonant> <wb>\n",
        "<wb> <short_vowel> <consonant> + <consonant> <long_vowel> <wb>\n",
        "<wb> <short_vowel> <consonant> + <consonant> <long_vowel> <consonant> <wb>\n",
        "<wb> <short_vowel> <consonant> <short_vowel> <consonant> + <consonant> <short_vowel> <consonant>\n",
        "<vowel> <n> + <consonant>\n",
        "<consonant> + <n> <aa> <wb>\n",
        "<consonant> + <n> <e> <wb>\n",
        "<consonant> + <t> <aa> <wb>\n",
        "<consonant> + <t> <e> <wb>\n",
        "<consonant> + <t> <ii> <wb>\n",
        "<consonant> + <ain> <short_vowel> <wb>\n",
        "<short_vowel> <ain> + <consonant>\n",
        "<wb> <consonant> <short_vowel> + <consonant> <long_vowel>\n",
        "<wb> <consonant> <short_vowel> <consonant> + <consonant> <long_vowel>\n",
        "<consonant> + <consonant>\n",
        " \u0924\u093e \n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "devanagari=myparser.Parser('devanagari.yaml')\n",
      "print devanagari.parse(\"vaqt\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<wb> <consonant> <short_vowel> <h_char> + <consonant> <wb>\n",
        "<wb> <consonant> <short_vowel> <consonant> + <consonant> <long_vowel> <wb>\n",
        "<wb> <consonant> <short_vowel> <consonant> + <consonant> <short_vowel> <consonant> <wb>\n",
        "<n> + <consonant>\n",
        "<consonant> + <consonant>\n",
        "<k> + <k_group>\n",
        "<ch> + <ch_group>\n",
        "<wb> <consonant> <long_vowel> <consonant> + <consonant> <long_vowel>\n",
        "<wb> <consonant> <long_vowel> <consonant> + <consonant> <short_vowel> <consonant>\n",
        "<wb> <consonant> <short_vowel> <sibilant> + <consonant> <wb>\n",
        "<wb> <consonant> <short_vowel> <consonant> + <consonant> <wb>\n",
        "<wb> <short_vowel> <consonant> + <consonant> <long_vowel> <wb>\n",
        "<wb> <short_vowel> <consonant> + <consonant> <long_vowel> <consonant> <wb>\n",
        "<wb> <short_vowel> <consonant> <short_vowel> <consonant> + <consonant> <short_vowel> <consonant>\n",
        "<vowel> <n> + <consonant>\n",
        "<consonant> + <n> <aa> <wb>\n",
        "<consonant> + <n> <e> <wb>\n",
        "<consonant> + <t> <aa> <wb>\n",
        "<consonant> + <t> <e> <wb>\n",
        "<consonant> + <t> <ii> <wb>\n",
        "<consonant> + <ain> <short_vowel> <wb>\n",
        "<short_vowel> <ain> + <consonant>\n",
        "<wb> <consonant> <short_vowel> + <consonant> <long_vowel>\n",
        "<wb> <consonant> <short_vowel> <consonant> + <consonant> <long_vowel>\n",
        "<wb> <long_vowel> <consonant> + <consonant> <long_vowel>\n",
        "<consonant> + <consonant>\n",
        "\u0935\u0915\u093c\u094d\u0924\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "devanagari.onmatch_rules"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "[((['wb', 'consonant', 'short_vowel', 'h_char'], ['consonant', 'wb']), ''),\n",
        " ((['wb', 'consonant', 'short_vowel', 'consonant'],\n",
        "   ['consonant', 'long_vowel', 'wb']),\n",
        "  ''),\n",
        " ((['wb', 'consonant', 'short_vowel', 'consonant'],\n",
        "   ['consonant', 'short_vowel', 'consonant', 'wb']),\n",
        "  ''),\n",
        " ((['n'], ['consonant']), ''),\n",
        " ((['consonant'], ['consonant']), u'\\u094d'),\n",
        " ((['k'], ['k_group']), u'\\u094d'),\n",
        " ((['ch'], ['ch_group']), u'\\u094d'),\n",
        " ((['wb', 'consonant', 'long_vowel', 'consonant'],\n",
        "   ['consonant', 'long_vowel']),\n",
        "  ''),\n",
        " ((['wb', 'consonant', 'long_vowel', 'consonant'],\n",
        "   ['consonant', 'short_vowel', 'consonant']),\n",
        "  ''),\n",
        " ((['wb', 'consonant', 'short_vowel', 'sibilant'], ['consonant', 'wb']),\n",
        "  u'\\u094d'),\n",
        " ((['wb', 'consonant', 'short_vowel', 'consonant'], ['consonant', 'wb']), ''),\n",
        " ((['wb', 'short_vowel', 'consonant'], ['consonant', 'long_vowel', 'wb']), ''),\n",
        " ((['wb', 'short_vowel', 'consonant'],\n",
        "   ['consonant', 'long_vowel', 'consonant', 'wb']),\n",
        "  ''),\n",
        " ((['wb', 'short_vowel', 'consonant', 'short_vowel', 'consonant'],\n",
        "   ['consonant', 'short_vowel', 'consonant']),\n",
        "  ''),\n",
        " ((['vowel', 'n'], ['consonant']), ''),\n",
        " ((['consonant'], ['n', 'aa', 'wb']), ''),\n",
        " ((['consonant'], ['n', 'e', 'wb']), ''),\n",
        " ((['consonant'], ['t', 'aa', 'wb']), ''),\n",
        " ((['consonant'], ['t', 'e', 'wb']), ''),\n",
        " ((['consonant'], ['t', 'ii', 'wb']), ''),\n",
        " ((['consonant'], ['ain', 'short_vowel', 'wb']), ''),\n",
        " ((['short_vowel', 'ain'], ['consonant']), ''),\n",
        " ((['wb', 'consonant', 'short_vowel'], ['consonant', 'long_vowel']), ''),\n",
        " ((['wb', 'consonant', 'short_vowel', 'consonant'],\n",
        "   ['consonant', 'long_vowel']),\n",
        "  ''),\n",
        " ((['wb', 'long_vowel', 'consonant'], ['consonant', 'long_vowel']), ''),\n",
        " ((['consonant'], ['consonant']), u'\\u094d')]"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "re.findall('<(.+?)>','< x > < x >< x >y')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 56,
       "text": [
        "[' x ', ' x ', ' x ']"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "re.match('((?:\\s?<.+?>\\s+)+)?', '<wb> <consonant> a h <wb>').group(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 50,
       "text": [
        "'<wb> <consonant> '"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "        rule={}\n",
      "        key = '<wb> <consonant> a h <br> <wb>'\n",
      "        _  ='(?:'   \n",
      "        _ +='\\('\n",
      "        _ +='((?:\\s?<.+?>\\s+)+)?'# '(?:\\s?<(.+?)>\\s+)?' # group 1, prev class (in brackets indicating cluster)\n",
      "        _ +='(.+?)\\s?' # group 2, prev tokens (to be split)\n",
      "        _ +='\\) '\n",
      "        _ +='|' # either a cluster or a particular previous class (Could add additional support, e.g. class or paretic.\n",
      "        _ +='((?:\\s?<.+?>\\s+)+)?' # group 3, prev class (not in cluster)\n",
      "        _ +=')?'\n",
      "        _ += '(.+?)' # group 4, tokens\n",
      "        _ += '(?:' # cluster for following tokens, clusters \n",
      "        _ += ' \\('\n",
      "        _ += '\\s?(.+?)' # group 5, next tokens\n",
      "        _ += '((?:\\s?<.+?>\\s+)+)?' # group 6, next class\n",
      "        _ += '\\s?\\)'\n",
      "        _ += '|'\n",
      "        _ += ' ((?:\\s?<.+?>\\s+)+)?' # group 7, follo\n",
      "        _ += ')?$'\n",
      "        print _\n",
      "        m = re.match (_, key, re.S)\n",
      "        assert (m is not None)\n",
      "        if m.group(1): rule['prev_class'] = re.findall('<(.+?)>',m.group(1))\n",
      "        if m.group(2): rule['prev_tokens'] = m.group(2).split(' ')\n",
      "        if m.group(3): rule['prev_class'] =  re.findall('<(.+?)>',m.group(3))\n",
      "        if m.group(4)==' ':\n",
      "            rule['tokens'] = ' '\n",
      "        else:\n",
      "            rule['tokens'] = m.group(4).split(' ')\n",
      "        if m.group(5): rule['next_tokens'] = m.group(5).split(' ')\n",
      "        if m.group(6): rule['next_class'] = re.findall('<(.+?)>',m.group(6))\n",
      "        if m.group(7): rule['next_class'] = re.findall('<(.+?)>',m.group(7))\n",
      "            \n",
      "        print rule"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(?:\\(((?:\\s?<.+?>\\s+)+)?(.+?)\\s?\\) |((?:\\s?<.+?>\\s+)+)?)?(.+?)(?: \\(\\s?(.+?)((?:\\s?<.+?>\\s+)+)?\\s?\\)| ((?:\\s?<.+?>\\s+)+)?)?$\n",
        "{'prev_class': ['wb', 'consonant'], 'tokens': ['a', 'h', '<br>', '<wb>']}\n"
       ]
      }
     ],
     "prompt_number": 70
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rule['prev_classes'] = ['vowela','vowelb']\n",
      "prev_classes = rule['prev_classes']\n",
      "tkns=['a','b','c']\n",
      "i_start=2\n",
      "to_match = [' ']+tkns[i_start-len(prev_class):i_start]\n",
      "to_match"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 89,
       "text": [
        "[' ', 'a', 'b']"
       ]
      }
     ],
     "prompt_number": 89
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in reversed(range(len(prev_classes))):\n",
      "    print i,to_match[i],prev_classes[i]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 a vowelb\n",
        "0   vowela\n"
       ]
      }
     ],
     "prompt_number": 90
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rule={'tokens': ['aa'], 'production': u'\\u0906', 'prev_classes': ['wb']}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 91
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}